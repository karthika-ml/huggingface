## Setup and Overview</br>
To get started, we import the necessary dependencies. Our workflow includes pushing the model to a Hugging Face account using huggingface_hub, tracking progress with Weights & Biases, and utilizing Hugging Face's transformers library along with the PEFT (Parameter-Efficient Fine-Tuning) library for fine-tuning with minimal data.</br>
The Hugging Face transformers library is conveniently integrated with Weights & Biases for seamless logging and tracking. For this project, we'll use the IMDb dataset, which includes two fields: labels (indicating whether a review is positive or negative) and text (the actual movie review). To ensure balanced classes, I truncated the dataset to 1,000 samples and visualized it to verify that both the training and testing partitions are evenly distributed.</br>
For model selection, we leverage the AutoModelForSequenceClassification class from the transformers library, which automatically fetches the appropriate model for our task. In this case, we've chosen distilbert-base-uncased. The data is preprocessed by tokenizing it, and we'll employ a data collator to batch the sequences and apply dynamic padding, optimizing the training speed. Weâ€™ll use the accuracy metric for evaluation, and implement LoRA as the PEFT method to fine-tune our model efficiently while maintaining high performance. </br>

Check out the detailed training report on [Weights & Biases](https://api.wandb.ai/links/karthika-ncsu-intelliswift-software/umdkk794).
![huggingfacellmfinetune](https://github.com/user-attachments/assets/3a9b8670-695d-46d2-857f-880087da8f48)
